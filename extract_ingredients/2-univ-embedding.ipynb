{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in docker:\n",
    "# sudo docker run -p 8890:8888 -v /home/tehdog/data/dev/2019/pic2kcal-cv-praktikum:/tf/notebooks --runtime=nvidia -it --rm tensorflow/tensorflow:1.12.0-gpu-py3 jupyter notebook --allow-root --notebook-dir=/tf/notebooks\n",
    "# crashes in TF1.4!\n",
    "# does not work with Arch Linux TF installation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.5/dist-packages (0.5.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-hub) (1.15.4)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-hub) (3.6.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-hub) (1.11.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.5/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (40.5.0)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.5/dist-packages (0.1.82)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tf-sentencepiece in /usr/local/lib/python3.5/dist-packages (0.1.82.1)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.5/dist-packages (4.32.2)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-hub\n",
    "!pip install sentencepiece\n",
    "!pip install tf-sentencepiece\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0701 10:25:24.051423 140250267367168 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import tf_sentencepiece\n",
    "\n",
    "# Set up graph.\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "  text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "  en_de_embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-xling/en-de/1\")\n",
    "  embedded_text = en_de_embed(text_input)\n",
    "  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "g.finalize()\n",
    "\n",
    "# Initialize session.\n",
    "session = tf.Session(graph=g)\n",
    "session.run(init_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import heapq\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from math import ceil\n",
    "\n",
    "data_dir = Path(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jq '.[]' fddb_data_v3.json > fddb_data_v3.jsonl\n",
    "# jq 'select(([.Bilder[]|select(.title != \"Noch kein Foto vorhanden.\")]|length) > 0)' fddb_data_v3.jsonl | jq -s > fddb_data_v3_withimg.json\n",
    "with open(str(data_dir / \"fddb_data_v3_withimg.json\"), encoding='utf-8') as f:\n",
    "    fddb = json.load(f)\n",
    "    # todo: make unique here\n",
    "    _out_names = [e[\"name\"] for e in fddb]\n",
    "\n",
    "# jq '.ingredients[]|select(.ingredient)|.ingredient' processed_data.jsonl | jq -s unique > ingredients.json\n",
    "with open(str(data_dir / \"recipes/ingredients.json\"), encoding='utf-8') as f:\n",
    "    _in_names = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_ingredient(ing: str):\n",
    "    ing = re.sub(r\"\\(([^)])\\)\", \"\\g<1>\", ing)  # remove stuff in parens\n",
    "    ing = re.sub(r\"\\([^)]+\\)\", \"\", ing)  # remove stuff in parens\n",
    "    ing = re.sub(r\"(\\d+,)?\\d+ k?g\\b\", \"\", ing)  # remove xyz gram\n",
    "    ing = re.sub(r\",.*\", \"\", ing)\n",
    "    ing = re.sub(r\"\\bzum .*\", \"\", ing)\n",
    "    ing = re.sub(r\"\\boder\\b.*\", \"\", ing)\n",
    "    ing = ing.strip()\n",
    "    return ing\n",
    "\n",
    "from extract_ingredients.util import normalize_out_ingredient\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21444"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_names = list({normalize_ingredient(ing) for ing in _in_names})\n",
    "print(len(_in_names))\n",
    "in_names.sort()\n",
    "len(in_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "108084"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(_out_names))\n",
    "out_names = list({ning for ing in _out_names for ning in normalize_out_ingredient(ing)})\n",
    "out_names.sort()\n",
    "len(out_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def Matcher():\n",
    "#    def __init__(self, data_left, data_right, preproc_left, preproc_right):\n",
    "#        pre_left = list(map(preproc_left, data_left))\n",
    "#        self.left_vecs = list(zip(data_left, pre_left, get_sentence_vectors(pre_left)))\n",
    "#        \n",
    "#        pre_right = list(map(preproc_right, data_right))\n",
    "#        self.right_vecs = list(zip(data_right, pre_right, get_sentence_vectors(pre_right)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return (l[i:i+n] for i in range(0, len(l), n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vectors(texts):\n",
    "    bs = 10000\n",
    "    ccount = len(texts)//bs\n",
    "    chunks = make_chunks(texts, bs)\n",
    "    if ccount >= 3:\n",
    "        chunks = tqdm(chunks, total=ccount)\n",
    "    for chunk in chunks:\n",
    "        yield from session.run(embedded_text, feed_dict={text_input: chunk})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match(search: np.array, out_vecs, limit=10):\n",
    "    it = ((v[0], np.dot(v[1], search)) for v in out_vecs)\n",
    "    res_list = heapq.nlargest(limit, it, key=itemgetter(1))\n",
    "    return res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245b14d35ab84107a49edf12707429c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_vecs = list(zip(out_names, get_sentence_vectors(out_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_vecs[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('8 Kräuter', '\"DIE FEINE\" Geflügel-Fleischwurst, würzi...')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_names[0], out_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vecs = list(zip(in_names, get_sentence_vectors(in_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sahnesteif für 4 dl Sahne -> ('Unser Sahnefest, Sahnesteif', 0.677456)\n",
      "Haselnussblättchen für die Form -> ('Haselnusskerne ganz', 0.8386623)\n",
      "Himbeeren und einige -> ('Himbeeren', 0.8851223)\n",
      "Sahne süss -> ('Sahne leicht', 0.8723918)\n",
      "Butter in kleinen Stücken -> ('Leichte Butter', 0.8694309)\n",
      "Kartoffelpüreepulver mit Milch -> ('Kartoffelpüree mit entrahmter Milch', 0.92771316)\n",
      "Butter und -> ('Butter', 0.8537569)\n",
      "Wildkräuter  mit essbaren Blüten -> ('Kräuterlinge zum Streuen, Frühlingskräuter', 0.8207296)\n",
      "Gewürze  nach Geschmack -> ('Gewürzmandeln', 0.8939085)\n",
      "Lasagneplatten bzw. Cannelloniröhrchen -> ('Lasagneplatten', 0.80853695)\n",
      "Mehl für Backstein/Backblech -> ('Leinschrotmehl, Leinmehl, Leinkuchen, entfettet', 0.81676424)\n",
      "Thunfisch mit Gemüse und Soße -> ('Thunfisch, mit Gemüse', 0.9462609)\n",
      "Blue Curaçao Sirup -> ('French Vanilla Sirup', 0.81558037)\n",
      "Schweineschnitzel von der Lende -> ('Minischnitzel vom Schwein', 0.8926196)\n",
      "Orangen - Mandarinen -> ('Mandarinen-Orangen', 0.8909749)\n",
      "Fischfilets vom weißen -> ('Weiße Thunfischfilets', 0.80376214)\n",
      "Kasseler - Kotelett -> ('Kasseler Kotelett', 0.9224388)\n",
      "Rum nach Geschmack -> ('Rum mit Cola', 0.8273617)\n",
      "Früchte der Saison. gemischte -> ('Obstplunder, Obst der Saison', 0.7268808)\n",
      "Schmelzkäse mit Kräuter -> ('Kochkäse mit Kräutern', 0.955901)\n",
      "Fett für die Auflaufform -> ('Speisestärke', 0.7981351)\n",
      "Thai-Currypaste  aus dem Asia-Laden -> ('ThaiCurry (selbstgemacht,)', 0.8995588)\n",
      "Jakobsmuscheln - Schalen -> ('Jakobsmuscheln', 0.91867393)\n",
      "Brezellauge -> ('Debrecziner', 0.8118499)\n",
      "Chilischoten . mittelscharf -> ('Chilischote', 0.78909004)\n",
      "Kokosnuss - Schnitzelchen -> ('Kokosnussschnitze', 0.9081713)\n",
      "Currypaste bei Bedarf -> ('Currygeschnetzeltes', 0.79391587)\n",
      "Basilikum und Oregano gemischt -> ('Oregano, getrocknet', 0.8519486)\n",
      "Vanille - Pudding -> ('Vanille -Shake', 0.8514253)\n",
      "Schweineschmalz und Butter -> ('Butterschmalz', 0.84130645)\n",
      "Pflanzenkohle -> ('Hanfmehl', 0.82263243)\n",
      "Vanille nach Belieben -> ('All In, Vanille', 0.77881145)\n",
      "Sherry fino -> ('Sherry', 0.83111525)\n",
      "Geflügelklein (Hals -> ('Geflügel-Paste', 0.88637114)\n",
      "Schinken am Stück -> ('Schinken-Speck', 0.88907397)\n",
      "Paniermehl ODER: -> ('Paniermehl', 0.7327008)\n",
      "Sonnenblumenkerne gemahlen -> ('Sonnenblumenkerne', 0.9438298)\n",
      "Himbeeren und Heidelbeeren -> ('Himbeeren', 0.8943845)\n",
      "Schinken - Reste -> ('Schinken-Wurst', 0.86084175)\n",
      "Gulasch vom Wildschwein -> ('Gulasch, vom Schwein', 0.90935814)\n",
      "Blüten aus Esspapier -> ('Blütenpollen', 0.80175126)\n",
      "Rotbarschfilets á -> ('Rotbarschfilet', 0.8324573)\n",
      "Hummer den Panzer davon -> ('Hummer-Paste', 0.74637306)\n",
      "Entenbrust á -> ('Entenbrust', 0.8524048)\n",
      "Kirschen in Weinbrand -> ('Sauerkirschen im Glas', 0.8544677)\n",
      "Zucker und 1/2 EL extra -> ('Süß-Sauer, ohne Zuckerzusatz', 0.8219334)\n",
      "Rum und Milch -> ('Rum Schokolade', 0.8546076)\n",
      "Zucker und 1 -> ('Gelier Zucker, 2 plus 1', 0.82239044)\n",
      "Zucker - Konfetti -> ('Zuckererbsen-Schoten', 0.81207716)\n",
      "Thunfisch in eigenem Saft und Aufguss -> ('Thunfisch im eigenen Saft und Aufguss', 0.9646826)\n",
      "Schweineschmalz für das Blech -> ('Schweine-Schmalz', 0.83047915)\n",
      "Tofu Rosso -> ('Tofu Basiliko', 0.8981621)\n",
      "Mangos in Streifen -> ('Mango Streifen', 0.93208694)\n",
      "Sauerteig aus dem Beutel -> ('Schmand aus Sauerrahm', 0.8069513)\n",
      "Kaffir-Zitronenblätter -> ('Kohlrabiblätter', 0.84735405)\n",
      "Rucola - Pesto -> ('Pesto Rucola', 0.8773942)\n",
      "Porree . klein geschnitten -> ('Porree geschnitten', 0.8324996)\n",
      "Wurst z B. Leberkäse -> ('Bio Wurst, Leberkäse', 0.8485607)\n",
      "Rinderfilets à ca. -> ('Geflügel-Cervelas, Poulet fleisch', 0.7238403)\n",
      "Fischbällchen  aus dem Asiashop -> ('Backfischstäbchen', 0.7489378)\n",
      "Semmelbrösel ODER Backpapier -> ('Knabber-Esspapier', 0.88786924)\n",
      "Sardellen aus der Dose -> ('Sardellen filet', 0.9081285)\n",
      "Honig und Zimt -> ('Honig mit Zimt', 0.9518355)\n",
      "Graubrot -> ('Graubrot', 1.0)\n",
      "Hackfleisch - Klößchen aus Putenfleisch -> ('Frisches Puten Hackfleisch', 0.8400548)\n",
      "Rindfleisch vom Nacken -> ('Nackenkotelett vom Schwein', 0.84258544)\n",
      "Vanilleschoten ODER: -> ('Vanilleschote', 0.658627)\n",
      "Zimt -Zucker -> ('Zimt-Kekse', 0.9054236)\n",
      "Ambarellas Fruchtfleisch -> ('Feinstes Tomaten Fruchtfleisch', 0.79191494)\n",
      "Pinienkerne und Mandeln -> ('Mandeln und Cranberries', 0.8884372)\n",
      "Pizzateig aus dem Kühlregal à -> ('Frischer Pizzateig', 0.8153674)\n",
      "Keulen vom Lamm ca  pro Stück -> ('Rinder-Gulasch aus der Keule', 0.7377274)\n",
      "Getreide  überbrüht mit kochendem Wasser -> ('Gemüsebrühepulver, Wasser', 0.8024584)\n",
      "Milch 2 -> ('Milch Extra', 0.8160862)\n",
      "Wasser für den Topf -> ('Wasser, still', 0.753086)\n",
      "Salz Pfeffer -> ('Pfeffer Soße', 0.8834938)\n",
      "Ofenkartoffeln -> ('Ofenkartoffeln', 1.0)\n",
      "Steaks  mit Knochen -> ('Schweinebauch mit Knochen', 0.8232862)\n",
      "Minutensteaks aus dem Schweinerücken -> ('Minutensteak, aus dem Schweinerücken', 0.95785856)\n",
      "Sahne und 1 Pck. Sahnesteif -> ('Sahne-Meerrettich, Sahnig Mild', 0.80487704)\n",
      "Schokolade mit Marzipanfüllung -> ('Marzipan mit Schokolade Ei', 0.8938064)\n",
      "Sojadrink -> ('Sojadrink', 1.0)\n",
      "Öl und -> ('Öl', 0.8959179)\n",
      "Kichererbsen - Mehl -> ('Kichererbsen  Mehl', 0.9257008)\n",
      "Limettenöl -> ('Leinöl', 0.92201734)\n",
      "Getreide - Flocken -> ('Haferkleie Flocken', 0.84273165)\n",
      "Aufbackbrötchen -> ('Aufbackbrötchen', 1.0)\n",
      "Mehl zun Ausrollen -> ('Süsslupinenmehl', 0.8366554)\n",
      "Schlagsahne 30% -> ('Schlagsahne 30%', 1.0)\n",
      "Quitten (Reste aus dem Dampfentsafter -> ('Quittensaft', 0.8037341)\n",
      "Lunge vom Kalb -> ('Saure Lunge', 0.83252454)\n",
      "Aceto balsamico di modena -> ('aceto balsamico, di modena', 0.9384121)\n",
      "Parmesan und Pesto -> ('Pesto, Tomate & Trüffel', 0.8604649)\n",
      "Tortillas . -> ('Tortillas', 0.80207825)\n",
      "Creme  aus der Dose -> ('Sour Creme', 0.85212076)\n",
      "Parmaschinken -> ('Parmaschinken', 0.9999998)\n",
      "Sauerteig : Stehzeit ca. 18 Std. -> ('Speisequark , 20tt i. Fr.', 0.6719196)\n",
      "Puderzucker und Jägermeister -> ('Puderzucker', 0.8066629)\n",
      "Puddingpulver für Mousse au Chocolat -> ('Mousse au Chocolat Muffin', 0.8459067)\n",
      "Dinkelgrieß -> ('Dinkelgrieß', 0.99999976)\n"
     ]
    }
   ],
   "source": [
    "for ingredient, vec in random.sample(in_vecs, 100):\n",
    "    res_list = get_match(vec, out_vecs)\n",
    "    print(\"{} -> {}\".format(ingredient, res_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Kartoffeln', 1.0),\n",
       " ('Kartoffelstock', 0.8959534),\n",
       " ('Kartoffeln roh', 0.8905239),\n",
       " ('Kartoffeln gebacken', 0.8904908),\n",
       " ('Kartoffelscheiben', 0.8901619),\n",
       " ('Kartoffeln, ganz', 0.88885343),\n",
       " ('Kartoffel', 0.88085693),\n",
       " ('Kartoffelnudeln', 0.8792579),\n",
       " ('Gekochte Kartoffeln', 0.8718474),\n",
       " ('geschälte Kartoffeln', 0.8708629)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = [inv for inv in in_vecs if inv[0] == 'Kartoffeln'][0]\n",
    "get_match(z[1], out_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Schalotten', 1.0000001),\n",
       " ('Sülze', 0.89274013),\n",
       " ('Ringlotten', 0.8788649),\n",
       " ('Schaschlik', 0.8767235),\n",
       " ('Leberkse', 0.87103057),\n",
       " ('Schwartenmagen', 0.86798453),\n",
       " ('Maiskölbchen', 0.86279553),\n",
       " ('Grützwust', 0.8598486),\n",
       " ('Maisecke', 0.85870075),\n",
       " ('Käsknacker', 0.858381),\n",
       " ('Hhnerfilet', 0.8569063),\n",
       " ('Korneck', 0.85655665),\n",
       " ('Körnereck', 0.8554502),\n",
       " ('Landschinken', 0.85422593),\n",
       " ('Rohschinken', 0.8509237),\n",
       " ('Karreespeck', 0.85052764),\n",
       " ('Wafer', 0.84980637),\n",
       " ('Kräcker', 0.8489746),\n",
       " ('Radieschen', 0.84836495),\n",
       " ('Rehgulasch', 0.8480122),\n",
       " ('Kareespeck', 0.8477373),\n",
       " ('Rohesser', 0.846316),\n",
       " ('Körnerbursche', 0.8454909),\n",
       " ('Mairübchen', 0.8453922),\n",
       " ('Rettich', 0.8451376),\n",
       " ('Jausenspeck', 0.8450292),\n",
       " ('Kaisersemmeln', 0.8447572),\n",
       " ('Stelline', 0.84436196),\n",
       " ('Palatschinken', 0.8443409),\n",
       " ('Mohnstuten', 0.84411025)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = list(get_sentence_vectors([\"Schalotten\"]))[0]\n",
    "get_match(z, out_vecs, limit=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Salz', 1.0),\n",
       " ('Salzgurken', 0.9089097),\n",
       " ('Gut Salzig', 0.90342623),\n",
       " ('Salzbrezeln', 0.8880923),\n",
       " ('Salzgurken, salzig', 0.8686831),\n",
       " ('Salzmandeln', 0.8676863),\n",
       " ('Salzbrezeln, Salzig', 0.8605131),\n",
       " ('Salzbrezel', 0.8582727),\n",
       " ('Salz-Mandeln', 0.85328484),\n",
       " ('salzbrezel', 0.84165347),\n",
       " ('Salz, Jodsalz', 0.8251066),\n",
       " ('Salzbutter', 0.81924367),\n",
       " ('Salzdillgurken', 0.8158475),\n",
       " ('Jod Salz', 0.81424797),\n",
       " ('Salz flutes, Salz', 0.8119249),\n",
       " ('Blutdruck Salz', 0.8090307),\n",
       " ('Salz-Stangen', 0.80218107),\n",
       " ('smorbar gesalzen', 0.7952043),\n",
       " ('Chips Salz', 0.79345155),\n",
       " ('Salzstangerl', 0.7885661),\n",
       " ('Salz Kräcker', 0.7803569),\n",
       " ('Corn, salz', 0.7801466),\n",
       " ('Salz Stangen', 0.7790837),\n",
       " ('Salzige Heringe', 0.7762423),\n",
       " ('Salzmandeln, geröstet gesalzen', 0.76485217),\n",
       " ('Streichzart, gesalzen', 0.76328605),\n",
       " ('Saltletts', 0.7625023),\n",
       " ('Salz-Schneidebohnen', 0.76154387),\n",
       " ('Salzstangen', 0.7579998),\n",
       " ('Salz-Cashews', 0.7565831)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = list(get_sentence_vectors([normalize_ingredient(\"Salz\")]))[0]\n",
    "get_match(z, out_vecs, limit=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c189f1fa024e0b91070fc6c20549ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=21444), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# i thought multiprocessing is impossible in jupyter notebook?\n",
    "# better not question it\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def jsonable(e):\n",
    "    return [(a,float(b)) for a, b in e]\n",
    "\n",
    "def get_match_w(t):\n",
    "    ing, vec = t\n",
    "    return ing, get_match(vec, out_vecs)\n",
    "\n",
    "all_ings = {}\n",
    "with Pool(8) as pool:\n",
    "    for ing, match in pool.imap(get_match_w, tqdm(in_vecs), chunksize=100):\n",
    "        all_ings[ing] = jsonable(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_ings_orig = {ing: all_ings[normalize_ingredient(ing)] for ing in _in_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/recipe-ingredient-to-fooddb.json\", \"w\") as f:\n",
    "    json.dump(all_ings_orig, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
