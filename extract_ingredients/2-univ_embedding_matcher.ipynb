{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in docker:\n",
    "# sudo docker run -p 8890:8888 -v /home/tehdog/data/dev/2019/pic2kcal-cv-praktikum:/tf/notebooks --runtime=nvidia -it --rm tensorflow/tensorflow:1.12.0-gpu-py3 jupyter notebook --allow-root --notebook-dir=/tf/notebooks\n",
    "# crashes in TF1.4!\n",
    "# does not work with Arch Linux TF installation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/notebooks\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.5/dist-packages (0.5.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-hub) (1.15.4)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-hub) (3.6.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-hub) (1.11.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.5/dist-packages (from protobuf>=3.4.0->tensorflow-hub) (40.5.0)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.5/dist-packages (0.1.82)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tf-sentencepiece in /usr/local/lib/python3.5/dist-packages (0.1.82.1)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.5/dist-packages (4.32.2)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-hub\n",
    "!pip install sentencepiece\n",
    "!pip install tf-sentencepiece\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0630 21:11:42.692018 139825671636736 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import tf_sentencepiece\n",
    "\n",
    "# Set up graph.\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "  text_input = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "  en_de_embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-xling/en-de/1\")\n",
    "  embedded_text = en_de_embed(text_input)\n",
    "  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "g.finalize()\n",
    "\n",
    "# Initialize session.\n",
    "session = tf.Session(graph=g)\n",
    "session.run(init_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import heapq\n",
    "from operator import itemgetter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from math import ceil\n",
    "\n",
    "data_dir = Path(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jq '.[]' fddb_data_v3.json > fddb_data_v3.jsonl\n",
    "# jq 'select(([.Bilder[]|select(.title != \"Noch kein Foto vorhanden.\")]|length) > 0)' fddb_data_v3.jsonl | jq -s > fddb_data_v3_withimg.json\n",
    "with open(str(data_dir / \"fddb_data_v3_withimg.json\"), encoding='utf-8') as f:\n",
    "    fddb = json.load(f)\n",
    "    # todo: make unique here\n",
    "    out_names = [e[\"name\"] for e in fddb]\n",
    "\n",
    "# jq '.ingredients[]|select(.ingredient)|.ingredient' processed_data.jsonl | jq -s unique > ingredients.json\n",
    "with open(str(data_dir / \"recipes/ingredients.json\"), encoding='utf-8') as f:\n",
    "    _in_names = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_ingredient(ing: str):\n",
    "    ing = re.sub(r\"\\(([^)])\\)\", \"\\g<1>\", ing)  # remove stuff in parens\n",
    "    ing = re.sub(r\"\\([^)]+\\)\", \"\", ing)  # remove stuff in parens\n",
    "    ing = re.sub(r\"(\\d+,)?\\d+ k?g\\b\", \"\", ing)  # remove xyz gram\n",
    "    ing = re.sub(r\",.*\", \"\", ing)\n",
    "    ing = re.sub(r\"\\bzum .*\", \"\", ing)\n",
    "    ing = re.sub(r\"\\boder\\b.*\", \"\", ing)\n",
    "    ing = ing.strip()\n",
    "    return ing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21444"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_names = list({normalize_ingredient(ing) for ing in _in_names})\n",
    "len(in_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122977"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def Matcher():\n",
    "#    def __init__(self, data_left, data_right, preproc_left, preproc_right):\n",
    "#        pre_left = list(map(preproc_left, data_left))\n",
    "#        self.left_vecs = list(zip(data_left, pre_left, get_sentence_vectors(pre_left)))\n",
    "#        \n",
    "#        pre_right = list(map(preproc_right, data_right))\n",
    "#        self.right_vecs = list(zip(data_right, pre_right, get_sentence_vectors(pre_right)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return (l[i:i+n] for i in range(0, len(l), n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vectors(texts):\n",
    "    bs = 10000\n",
    "    ccount = len(texts)//bs\n",
    "    chunks = make_chunks(texts, bs)\n",
    "    if ccount >= 3:\n",
    "        chunks = tqdm(chunks, total=ccount)\n",
    "    for chunk in chunks:\n",
    "        yield from session.run(embedded_text, feed_dict={text_input: chunk})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match(search: np.array, out_vecs):\n",
    "    it = ((v[0], np.dot(v[1], search)) for v in out_vecs)\n",
    "    res_list = heapq.nlargest(10, it, key=itemgetter(1))\n",
    "    return res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57e1a218b304c9fbb1e972baf6a4416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_vecs = list(zip(out_names, get_sentence_vectors(out_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_vecs[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('HENGLEIN frischer Flammkuchenteig', 'Get Fruity Bar, Marvellous Mango')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_names[0], out_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vecs = list(zip(in_names, get_sentence_vectors(in_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piment d’Espelette -> ('Petit Frais au Curcuma', 0.86310613)\n",
      "Baiser - Boden -> ('Ovalis - Paradeiser', 0.7531158)\n",
      "Salsa Dip -> ('Salsa Dip', 1.0000001)\n",
      "Smarties o. ä. -> ('Smarties', 0.774335)\n",
      "WanTan - Teigblätter -> ('Teigblätter', 0.7666634)\n",
      "Mandelstifte -> ('Mandelbällchen', 0.8798618)\n",
      "Crème fraîche Legere -> ('Crème fraîche', 0.969875)\n",
      "Dörrfleisch nach Belieben -> ('Dörrpflaumen', 0.8018552)\n",
      "Hanfsamen -> ('Hanfsamen', 1.0)\n",
      "Gehacktes / Mett -> ('Dietzel Mettenden', 0.7645625)\n",
      "Gemüsebrühe aus den Schalen des Gemüses -> ('Gemüsebrühe, aus frischen zutaten', 0.9037679)\n",
      "Zucker 2 -> ('Zucker', 0.86964095)\n",
      "Sahnemeerrettich -> ('Sahnemeerrettich', 1.0)\n",
      "Putenbrust -> ('Putenbrust', 1.0)\n",
      "Salz und Pfeffer ... -> ('Pfeffer & Salz Chips, Pfeffer und Salz', 0.73837936)\n",
      "Fleischbrühe von den Rippchen -> ('Rinderknochenbrühe', 0.859671)\n",
      "Mini-Mozzarella Kugeln -> ('Mozzarella Kugel', 0.9120718)\n",
      "Fischfond - Paste -> ('Fischfond', 0.8705851)\n",
      "Zucker - Austauschstoff -> ('Flüssigsüßstoff', 0.7813295)\n",
      "Morcheln -> ('Morcheln', 1.0000002)\n",
      "Sellerie - Kraut -> ('Miree - Champignon', 0.80834126)\n",
      "Fruchtsüße -> ('Fruchtriese', 0.9467532)\n",
      "Basilikum und Bärlauch -> ('Basilikum', 0.8070309)\n",
      "Holz -> ('Eisen', 0.76968795)\n",
      "Fenchelsamen und Koriander -> ('Marinierter Fisch, Mediterrane Kräuter', 0.7989738)\n",
      "Grünkohl aus der Dose -> ('Grünkohl', 0.91145897)\n",
      "Soja - Cream -> ('Soja Mehl ', 0.8228935)\n",
      "Buttergemüse mit Butter-Kräuter-Sauce -> ('Buttergemüse, mit feiner Butter-Kräuter-Sauce', 0.97425085)\n",
      "Lebensmittelfarbe und 6 Cakepop- -> ('Blue Cheeze Cake', 0.58065116)\n",
      "Salz und Pfeffer + Tomatensalz -> ('Pfeffer & Salz Chips, Pfeffer und Salz', 0.86828923)\n",
      "Gans von ca. -> ('Gänse-Schmalz', 0.6221327)\n",
      "Sojamilch  bzw. Sojadrink -> ('Sojamilch', 0.8824947)\n",
      "Kirschen entkernt -> ('Kirschgrütze', 0.90216196)\n",
      "Heidelbeeren für die Garnitur -> ('Heidelbeeren ungesüßt', 0.8289746)\n",
      "Rotwein und/ -> ('Rotwein, lieblich', 0.87219405)\n",
      "Maggi für Ungar. Gulasch -> ('Maggi-Klare Fleischsuppe', 0.81392205)\n",
      "Butter und Mehl für die Förmchen -> ('Blätterbrezeln, zartes Buttergebäck', 0.79214895)\n",
      "Sauerkirsche Fruchtnektar -> ('Sauerkirsche Fruchtaufstrich', 0.94296026)\n",
      "Kapern mit Sud -> ('Kapern ', 0.80036575)\n",
      "Kreuzkümmel und Korinder -> ('Zöpfli, mit Kümmel und Salz', 0.85819304)\n",
      "Pfirsichkonfitüre -> ('Pfirsich-Konfitüre', 0.9663424)\n",
      "Gemüsebrühe aus Karotten -> ('Gemüsepuffer aus Karotten', 0.9251291)\n",
      "Eiswürfel bei Bedarf -> ('Eiswürfel', 0.850845)\n",
      "Saucenbinder für Rahmsaucen -> ('Rahm Soße', 0.8188437)\n",
      "Aroma Bittermandelaroma -> ('Bitter-Mandel-Aroma', 0.8375881)\n",
      "Schinkenwürfel mit Speck -> ('Schinken-Speckwürfel', 0.88299114)\n",
      "Flecke -> ('Fleckerl', 0.90470064)\n",
      "Gemüse z.B. Paprika -> ('Pflanzliche Pastete, Paprika', 0.8457606)\n",
      "Maisstärke z. B. Maizena -> ('Maisstärke Patissier', 0.83835423)\n",
      "Ziegenkäse mit Bockshornklee -> ('Ziegenkäse mit Bockshornklee ', 1.0)\n",
      "Gewürzmischung Harissa -> ('Harissa, scharfe Würzpaste', 0.87699413)\n",
      "Artischocken - Böden -> ('Artischockenböden', 0.8916345)\n",
      "Milch  vom Bauernhof -> ('Weidemilch', 0.8308588)\n",
      "Tempeh -> ('Tempeh', 1.0)\n",
      "Hirtenkäse aus Schafsmilch -> ('Hirtenkäse aus Kuhmilch', 0.9767034)\n",
      "Kokosraspel kurz rösten in einer trockenen Pfanne. -> ('Kichererbseneintopf, mit Kokosmilch', 0.81309974)\n",
      "Butter für die Kartoffeln in der Pfanne -> ('Teig für Kartoffel-Knödel', 0.8144232)\n",
      "Currypaste rot -> ('Curry Paste rot', 0.9453666)\n",
      "Zucker 1 -> ('Zucker', 0.89726996)\n",
      "Cremepulver für Paradiescreme Vanille -> ('Paradiescreme Vanille, Trockenprodukt', 0.9041972)\n",
      "Mehl  am besten frisch gemahlenes -> ('rohes Leinsamenmehl', 0.816615)\n",
      "Blumenkohl ca.  ohne Blätter -> ('Blumenkohl überbacken, Blumenkohl', 0.85274315)\n",
      "Karotten - Mousse: -> ('Karottencrèmesuppe', 0.73760515)\n",
      "Suppenpulver ** -> ('Zwiebelsuppe, nur  das Pulver', 0.74649805)\n",
      "Korianderpulver -> ('Volleipulver', 0.8720039)\n",
      "Saucenpulver für Kräutersauce -> ('Knoblauch Sauce, Mit feinen Kräutern ', 0.8400164)\n",
      "Soja - Nuggets -> ('Soja-Nuggets', 0.84081376)\n",
      "Zitronensaft / Zitrovin -> ('Zitronensaft', 0.8638614)\n",
      "Schmelzkäse - Scheiben -> ('Schmelzkäse Scheiben', 0.9663296)\n",
      "Spätzle  ODER: -> ('Spätzle', 0.78535396)\n",
      "Fondant für die Dekoration -> ('Fondant', 0.7498197)\n",
      "Essig - Wasser -> ('Essig', 0.8658979)\n",
      "Zitronat und/ -> ('Zitronat', 0.83782566)\n",
      "Fett für Schüssel und Form -> ('Fette Brühe', 0.73674846)\n",
      "Erdnussbutter mit Chili -> ('Erdnuss Creme, mit Karamell', 0.87746704)\n",
      "Rote Bete  + den Saft -> ('Rote  Bete Saft', 0.90000916)\n",
      "Obst wie z.B. Mango -> ('Fruit Smoothie, Apfel, Banane, Mango u.a.', 0.7830775)\n",
      "Henglein Kartoffelpufferteig -> ('Kartoffelpufferteig', 0.8942678)\n",
      "Braten vom Ochsen -> ('Braten vom Wildschwein', 0.85368)\n",
      "Toastbrot American Sandwich -> ('American Sandwich Toast', 0.9704818)\n",
      "Gelee - Bananen -> ('Karotten - Saft', 0.7673515)\n",
      "Mehl und Mehl -> ('Mehl', 0.9054698)\n"
     ]
    }
   ],
   "source": [
    "for ingredient, vec in random.sample(in_vecs, 100):\n",
    "    res_list = get_match(vec, out_vecs)\n",
    "    print(\"{} -> {}\".format(ingredient, res_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = [inv for inv in in_vecs if inv[0] == 'Kartoffeln'][0]\n",
    "get_match(z[1], out_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "def get_match_w(t):\n",
    "    ing, vec = t\n",
    "    return ing, get_match(vec, out_vecs)\n",
    "\n",
    "all_ings = {}\n",
    "with Pool(8) as pool:\n",
    "    for ing, match in pool.imap(get_match_w, tqdm(in_vecs), chunksize=100):\n",
    "        all_ings[ing] = match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonable(e):\n",
    "    return [(a,float(b)) for a, b in e]\n",
    "all_ings_orig = {ing: jsonable(all_ings[normalize_ingredient(ing)]) for ing in _in_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/recipe-ingredient-to-fooddb.json\", \"w\") as f:\n",
    "    json.dump(all_ings_orig, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ov for ov in out_vecs if ov[0] == 'Butter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match(\"^Nährwerte für 100 (\\w+)$\", \"Nährwerte für 100 g\").groups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "fddb_entries = defaultdict(list)\n",
    "rem = re.compile(r\"^(\\d+ )?(.*) \\((\\d+(?:,\\d+)?) (g|ml)\\)$\")\n",
    "for e in fddb:\n",
    "    fddb_entries[e[\"name\"]].append(e)\n",
    "    sanity = list(e[\"Standard Nährwerte\"].keys())\n",
    "    snmatch = re.match(\"^Nährwerte für 100 (\\w+)$\", sanity[0])\n",
    "    if snmatch is not None:\n",
    "        unn = snmatch.groups()[0]\n",
    "        e[\"parsed\"] = {\"unit\": unn, \"for_100\": e[\"Standard Nährwerte\"][\"Nährwerte für 100 {}\".format(unn)]}\n",
    "        \n",
    "    else:\n",
    "        print(\"SANITY ERROR:\", e[\"Id\"], sanity, unit)\n",
    "        continue\n",
    "    amts = e[\"parsed\"][\"amounts\"] = []\n",
    "    for k in e[\"Spezifische Nährwerte\"].keys():\n",
    "        if k == \"100 g (100 g)\" or k == \"100 g (100 ml)\" or k == \"100 ml (100 ml)\":\n",
    "            continue\n",
    "        txtamount, txt, amount, unit = rem.match(k).groups()\n",
    "        amount = float(amount.replace(\",\", \".\"))\n",
    "        txtamount = float(txtamount.replace(\",\", \".\")) if txtamount else 1.0\n",
    "        amts.append((txt, k, amount/txtamount))\n",
    "        \n",
    "        if unn != unit:\n",
    "            raise Exception(\"noooo\")\n",
    "        #print(txt, amount/txtamount, unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get recipe meta\n",
    "with open(\"data/recipes/processed_data.json\", encoding=\"utf-8\") as f:\n",
    "    recipes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get amount map from parse-amounts\n",
    "with open(\"data/recipes/parsed_amounts.json\") as f:\n",
    "    parsed_amounts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only recipes with pictures\n",
    "recipes = [recipe for recipe in recipes if len(recipe['picture_files']) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "# ugly af code\n",
    "debug=False\n",
    "for recipe in random.sample(recipes, 10):\n",
    "    print(\"----------------------------\")\n",
    "    print(\"\" + recipe[\"title\"])\n",
    "    for ingredient in recipe[\"ingredients\"]:\n",
    "        if \"amount\" in ingredient:\n",
    "            print((ingredient[\"amount\"].rjust(15) + \" \" + ingredient[\"ingredient\"]).ljust(55), end='')\n",
    "            print(\" = \", end='')\n",
    "            _z = parsed_amounts[ingredient[\"amount\"]]\n",
    "            if _z is None:\n",
    "                print(ingredient[\"amount\"])\n",
    "            amount_count, amount_type = _z\n",
    "            \n",
    "            if amount_count is None:\n",
    "                print(\" [ignored]\")\n",
    "                continue\n",
    "            if amount_type is None:\n",
    "                amount_type = \"Stück\"\n",
    "            #print(\"[\" + str(amount_count) + \"*\" + (amount_type)+ \"] of \", end=\"\")\n",
    "            _ing_matches = all_ings_orig[ingredient[\"ingredient\"]]\n",
    "            ing_matches = [e for e in _ing_matches if e[1] > 0.85]\n",
    "            if len(ing_matches) == 0:\n",
    "                # not so great match but eh\n",
    "                ing_matches = [_ing_matches[0]]\n",
    "            possible_entries = {entry[\"Id\"]: (ing_match[1], entry) for ing_match in ing_matches for entry in fddb_entries[ing_match[0]]}\n",
    "            possible_entries = sorted(list(possible_entries.values()), key=lambda e: -e[0])\n",
    "\n",
    "            debug and print(\"matches\", ing_matches)\n",
    "            debug and print(\"finding unit \", amount_type)\n",
    "            match = None\n",
    "            for accuracy, e in possible_entries:\n",
    "                if \"parsed\" not in e:\n",
    "                    continue\n",
    "                debug and print(\"considering\", e[\"name\"], e[\"Id\"])\n",
    "                have_unit = [amount_type]\n",
    "                want_unit = e[\"parsed\"][\"unit\"]\n",
    "                \n",
    "                if want_unit in have_unit:\n",
    "                    debug and print(\"got direct match!\", amount_count, have_unit, amount_count, want_unit)\n",
    "                    match = (e, accuracy, amount_count, want_unit, amount_count, want_unit)\n",
    "                    break\n",
    "                for amount_name, amount_source, to_norm in e[\"parsed\"][\"amounts\"]:\n",
    "                    debug and print(e[\"name\"], \"amount\", amount_source)\n",
    "                    if amount_name in have_unit:\n",
    "                        debug and print(\"got match unit!\", amount_source)\n",
    "                        match = (e, accuracy, amount_count, have_unit[0], amount_count*to_norm, want_unit)\n",
    "                        break\n",
    "                if match is not None:\n",
    "                    break\n",
    "            if not match:\n",
    "                print(\"[nomatch]\")\n",
    "            else:\n",
    "                fddb_entry, accuracy, count_weird, unit_weird, count_norm, unit_norm = match\n",
    "                if unit_weird == unit_norm:\n",
    "                    unittxt = \"{}{}\".format(count_norm, unit_norm)\n",
    "                else:\n",
    "                    unittxt = \"{} {} = {}{}\".format(count_weird, unit_weird, count_norm, unit_norm)\n",
    "                values = e[\"parsed\"][\"for_100\"]\n",
    "                value_kcal_per_100 = float(values[\"Kalorien\"][\"Menge\"].replace(\",\", \".\"))\n",
    "                value_kcal = value_kcal_per_100 / 100 * count_norm\n",
    "                print(\"({:.0f}%) [{}] of {} = {:.0f} kcal\".format(accuracy * 100, unittxt, fddb_entry[\"name\"], value_kcal))\n",
    "        else:\n",
    "            print(\"->\" + ingredient[\"subtitle\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
