from nn.dataset import FoodDataset
import math
import random
from torch.utils.data import DataLoader
import torch.nn as nn
import torch
import argparse
from collections import defaultdict
from nn.models.dense_net import DenseNet
from nn.models.res_net101 import ResNet101
from nn.models.res_net50 import ResNet50
from nn.models.baseline_model import BaselineModel
from pathlib import Path

# https://github.com/microsoft/ptvsd/issues/943
import multiprocessing

multiprocessing.set_start_method("spawn", True)


def count_parameters(model):
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total = sum(p.numel() for p in model.parameters())
    return trainable, total






def train():
    parser = argparse.ArgumentParser()
    parser.add_argument("--runname", help="name this experiment", required=True)
    parser.add_argument("--datadir",
                        help="input data dir generated by data/split.py (contains e.g. train.json and train/",
                        required=True)
    args = parser.parse_args()
    datadir = Path(args.datadir)
    batch_size = 50
    shuffle = True

    # training_type = 'classification'
    # training_type = 'regression'
    # training_type = 'regression_include_nutritional_data'
    training_type = 'regression_include_nutritional_data_and_top_top_ingredients'

    # regression settings
    regression_output_neurons = 1
    num_top_ingredients = 50

    # classification settings
    granularity = 50
    max_val = 2500

    def criterion_l1_loss_classif(a, b):
        ax = a.argmax(1).float()
        bx = b.float()

        # ax[torch.isnan(ax)] = 0
        # bx[torch.isnan(bx)] = 0
        return nn.L1Loss()(ax, bx) * granularity

    def criterion_rel_error(pred, truth):
        # https://en.wikipedia.org/wiki/Approximation_error
        ret = torch.abs(1 - pred / truth)
        ret[torch.isnan(ret)] = 0  # if truth = 0 relative error is undefined
        return torch.mean(ret)

    def loss_top_ingredients(pred, data):
        from torch.nn.functional import smooth_l1_loss, binary_cross_entropy_with_logits

        # todo: loop over enumerate(prediction_keys) here
        l1 = smooth_l1_loss(pred[:, 0:1], data["kcal"])
        l1 += smooth_l1_loss(pred[:, 1:2], data["protein"])
        l1 += smooth_l1_loss(pred[:, 2:3], data["fat"])
        l1 += smooth_l1_loss(pred[:, 3:4], data["carbohydrates"])
        if training_type == "regression_include_nutritional_data_and_top_top_ingredients":
            # todo: adjust the 400 factor to 2000 if per recipe etc
            bce = binary_cross_entropy_with_logits(pred[:, 4:], data["ingredients"]) * 400
            if random.random() < 0.1:
                print("l1 vs bce weight", float(l1), float(bce))
            return l1 + bce
        return l1

    loss_fns = {}

    prediction_keys = ["kcal"]

    if training_type == 'classification':
        is_regression = False
        num_output_neurons = math.ceil(max_val / granularity) + 1

        loss_fns["loss"] = nn.CrossEntropyLoss()
        loss_fns["l1_loss"] = criterion_l1_loss_classif
        # rel_error = None # TODO
    else:
        is_regression = True
        num_output_neurons = regression_output_neurons

        loss_fns["loss"] = lambda pred, data: nn.functional.smooth_l1_loss(pred, data["kcal"])
        loss_fns["l1_kcal"] = lambda pred, data: nn.functional.l1_loss(pred, data["kcal"])
        loss_fns["rel_error_kcal"] = lambda pred, data: criterion_rel_error(pred, data["kcal"])
    if training_type.startswith('regression_include_nutritional_data'):
        prediction_keys = ["kcal", "protein", "fat", "carbohydrates"]
        num_output_neurons += 3
        loss_fns["loss"] = loss_top_ingredients
        from torch.nn.functional import l1_loss
        def mk_loss(i, k, fn):
            def fuck_python(pred, data):
                return fn(pred[:, i:(i + 1)], data[k])

            return fuck_python

        for i, k in enumerate(prediction_keys):
            loss_fns[f"l1_{k}"] = mk_loss(i, k, l1_loss)
            loss_fns[f"rel_error_{k}"] = mk_loss(i, k, criterion_rel_error)
        i = 999
    if training_type == 'regression_include_nutritional_data_and_top_top_ingredients':
        num_output_neurons += num_top_ingredients




    model = DenseNet(num_output_neurons)  #
    print("model:", model.name)

    net = model.get_model_on_device(True)
    model.load('./weights/' + args.runname + '.pt')

    print(net)
    device = model.get_device()

    test_dataset = FoodDataset(datadir / "train.example.json", datadir / "train", is_regression, granularity, True,
                                True)

    test_loader = DataLoader(
        test_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4
    )

    trainable_params, total_params = count_parameters(net)
    print(f"Parameters: {trainable_params} trainable, {total_params} total")
    running_losses = defaultdict(list)


    with torch.no_grad():
        for epoch_batch_idx, data in enumerate(test_loader, 0):
            image_ongpu = data["image"].to(device)

            outputs = net(image_ongpu)

            target_data = {k: v.to(device) for k, v in data.items() if k != "image"}

            for loss_name, loss_fn in loss_fns.items():
                loss_value = loss_fn(outputs, target_data)
                running_losses[loss_name].append(float(loss_value.item()))

                print(loss_value)






if __name__ == "__main__":
    with torch.autograd.detect_anomaly():
        train()
