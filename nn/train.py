from nn.dataset import FoodDataset
import math
import random
from torch.utils.data import DataLoader
import torch.nn as nn
import torch
import datetime
import torch.optim as optim
import argparse
import numpy as np
from itertools import islice
from collections import defaultdict
from torch.utils.tensorboard import SummaryWriter
from nn.models.dense_net import DenseNet
from nn.models.res_net101 import ResNet101
from nn.models.res_net50 import ResNet50
from nn.models.baseline_model import BaselineModel
from pathlib import Path

# https://github.com/microsoft/ptvsd/issues/943
import multiprocessing

multiprocessing.set_start_method("spawn", True)


def count_parameters(model):
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total = sum(p.numel() for p in model.parameters())
    return trainable, total


def put_text(imgs, texts):
    result = np.empty_like(imgs)
    for i, (img, text) in enumerate(zip(imgs, texts)):
        from PIL import Image
        from PIL import ImageFont
        from PIL import ImageDraw

        img = Image.fromarray((img.transpose((1, 2, 0)) * 255).astype("uint8"), "RGB")
        draw = ImageDraw.Draw(img)
        # font = ImageFont.truetype(<font-file>, <font-size>)
        # font = ImageFont.truetype("/usr/share/fonts/TTF/LiberationSans-Regular.ttf", 16)
        # draw.text((x, y),"Sample Text",(r,g,b))
        draw.text((0, 0), text, (255, 255, 255))
        result[i] = (np.asarray(img).astype("float32") / 255).transpose((2, 0, 1))
        result[i] = result[i] - result[i].min() / (result[i].max() - result[i].min())
    return result




def write_losses(
    *, writer, running_losses, epoch, batch_idx, epoch_batch_idx, prefix=""
):
    for loss_name, running_loss in running_losses.items():
        avg_loss = np.mean(running_loss)
        loss_name_prefixed = f"{prefix}{loss_name}"
        writer.add_scalar(loss_name_prefixed, avg_loss, batch_idx)
        print(
            "[%d, %5d] %s: %.3f"
            % (epoch, epoch_batch_idx, loss_name_prefixed, avg_loss)
        )




def train():
    parser = argparse.ArgumentParser()
    parser.add_argument("--runname", help="name this experiment", required=True)
    parser.add_argument("--datadir",
                        help="input data dir generated by data/split.py (contains e.g. train.json and train/",
                        required=True)
    args = parser.parse_args()
    datadir = Path(args.datadir)
    batch_size = 50
    epochs = 20
    shuffle = True
    validate_every = 100
    validate_batches = 50
    show_img_count = 16

    # training_type = 'classification'
    # training_type = 'regression'
    # training_type = 'regression_include_nutritional_data'
    training_type = 'regression_include_nutritional_data_and_top_top_ingredients'
    


    # regression settings
    regression_output_neurons = 1
    num_top_ingredients = 50


    # classification settings
    granularity = 50
    max_val = 2500

    def criterion_l1_loss_classif(a, b):
        ax = a.argmax(1).float()
        bx = b.float()

        # ax[torch.isnan(ax)] = 0
        # bx[torch.isnan(bx)] = 0
        return nn.L1Loss()(ax, bx) * granularity


    def criterion_rel_error(pred, truth):
        # https://en.wikipedia.org/wiki/Approximation_error
        ret = torch.abs(1 - pred / truth)
        ret[torch.isnan(ret)] = 0 # if truth = 0 relative error is undefined
        return torch.mean(ret)
    
    def loss_top_ingredients(pred, data):
        from torch.nn.functional import smooth_l1_loss, binary_cross_entropy_with_logits

        l1 = smooth_l1_loss(pred[:, 0:1], data["kcal"])
        l1 += smooth_l1_loss(pred[:, 1:2], data["protein"])
        l1 += smooth_l1_loss(pred[:, 2:3], data["fat"])
        l1 += smooth_l1_loss(pred[:, 3:4], data["carbohydrates"])
        if training_type == "regression_include_nutritional_data_and_top_top_ingredients":
            bce = binary_cross_entropy_with_logits(pred[:, 4:], data["ingredients"]) * 400
            if random.random() < 0.1:
                print("l1 vs bce weight", float(l1), float(bce))
            return l1 + bce
        return l1

    loss_fns = {}

    if training_type == 'classification':
        is_regression = False
        num_output_neurons = math.ceil(max_val / granularity) + 1

        loss_fns["loss"] = nn.CrossEntropyLoss()
        loss_fns["l1_loss"] = criterion_l1_loss_classif
        # rel_error = None # TODO
    else:
        is_regression = True
        num_output_neurons = regression_output_neurons

        loss_fns["loss"] = lambda pred, data: nn.functional.smooth_l1_loss(pred, data["kcal"])
        loss_fns["l1_kcal"] = lambda pred, data: nn.functional.l1_loss(pred, data["kcal"])
        loss_fns["rel_error_kcal"] = lambda pred, data: criterion_rel_error(pred, data["kcal"])

    if training_type.startswith('regression_include_nutritional_data'):
        num_output_neurons += 3
        loss_fns["loss"] = loss_top_ingredients
        from torch.nn.functional import l1_loss
        for i, k in enumerate(["kcal", "protein", "fat", "carbohydrates"]):
            loss_fns[f"l1_{k}"] = lambda pred, data: l1_loss(pred[:, i:i+1], data[k])
            loss_fns[f"rel_error_{k}"] = lambda pred, data: criterion_rel_error(pred[:, i:i+1], data[k])

    if training_type == 'regression_include_nutritional_data_and_top_top_ingredients':
        num_output_neurons += num_top_ingredients

        
        


    logdir = (
        "runs/"
        + datetime.datetime.now().replace(microsecond=0).isoformat().replace(":", ".")
        + "-"
        + args.runname
    )
    writer = SummaryWriter(logdir)
    print(f"tensorboard logdir: {writer.log_dir}")

    model = DenseNet(num_output_neurons) # 
    print("model:", model.name)

    net = model.get_model_on_device(True)
    print(net)
    device = model.get_device()

    train_dataset = FoodDataset(datadir / "train.json", datadir / "train", is_regression, granularity, True, True)
    val_dataset = FoodDataset(datadir / "val.json", datadir / "val", is_regression, granularity, True, True)

    optimizer = optim.Adam(net.parameters())



    trainable_params, total_params = count_parameters(net)
    print(f"Parameters: {trainable_params} trainable, {total_params} total")
    running_losses = defaultdict(list)
    batch_idx = 0

    for epoch in range(1, epochs + 1):
        if epoch == 3:
            for param in net.parameters():
                param.requires_grad = True
            trainable_params, total_params = count_parameters(net)
            print('all params unfreezed')
            print(f"Parameters: {trainable_params} trainable, {total_params} total")

        train_loader = DataLoader(
            train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4
        )
        val_loader = DataLoader(
            val_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4
        )

        for epoch_batch_idx, data in enumerate(train_loader, 0):
            batch_idx += 1
            image_ongpu = data["image"].to(device)
            optimizer.zero_grad()

            outputs = net(image_ongpu)

            # print("out", outputs.shape)

            target_data = {k: v.to(device) for k, v in data.items() if k != "image"}

            for loss_name, loss_fn in loss_fns.items():
                loss_value = loss_fn(outputs, target_data)
                if loss_name == "loss":
                    loss_value.backward()
                    optimizer.step()
                # technically, this is not 100% correct because it assumes all batches are the same size
                running_losses[loss_name].append(float(loss_value.item()))

            if batch_idx % validate_every == 0:
                write_losses(
                    writer=writer,
                    running_losses=running_losses,
                    epoch=epoch,
                    batch_idx=batch_idx,
                    epoch_batch_idx=epoch_batch_idx,
                )
                running_losses = defaultdict(list)

            if batch_idx % validate_every == 0:
                val_error = defaultdict(list)
                # validation loop
                with torch.no_grad():
                    for data in islice(val_loader, validate_batches):
                        image = data["image"].to(device)
                        # print(data["image"], type(data["image"]))
                        kcal_cpu = data["kcal"] if is_regression else data["kcal"].squeeze()
                        kcal = kcal_cpu.to(device)

                        target_data = {k: v.to(device) for k, v in data.items() if k != "image"}

                        output = net(image)
                        for loss_name, loss_fn in loss_fns.items():
                            val_error[loss_name].append(float(loss_fn(output, target_data).item()))
                        
                        truth, pred = (
                            kcal_cpu.squeeze().numpy(),
                            output[:, 0:1].cpu().squeeze().numpy() if is_regression else torch.argmax(output.cpu(), 1).numpy(),
                        )
                    # only run this on last batch from val loop (truth, pred will be from last iteration)
                    images_cpu = (
                        image.view(-1, 3, 224, 224)[:show_img_count].cpu().numpy()
                    )

                    images_cpu = put_text(
                        images_cpu,
                        [
                            (f"truth: {t:.0f}kcal, pred: {p:.0f}kcal" if is_regression else f"truth: {t*granularity}kcal, pred: {p*granularity}kcal")
                            for t, p in zip(truth, pred)
                        ],
                    )
                    writer.add_images("YOOO", images_cpu)
                write_losses(
                    writer=writer,
                    running_losses=val_error,
                    epoch=epoch,
                    batch_idx=batch_idx,
                    epoch_batch_idx=epoch_batch_idx,
                    prefix="val_",
                )

    writer.close()
    model.save()


if __name__ == "__main__":
    with torch.autograd.detect_anomaly():
        train()
