{
	"https://dl.acm.org/citation.cfm?id=3297871": {
		"fetched": "2019-08-02T18:38:03.513Z",
		"bibtex": "\n@inproceedings{chokr_calories_2017,\n\taddress = {San Francisco, California, USA},\n\tseries = {{AAAI}'17},\n\ttitle = {Calories {Prediction} from {Food} {Images}},\n\turl = {http://dl.acm.org/citation.cfm?id=3297863.3297871},\n\tabstract = {Calculating the amount of calories in a given food item is now a common task. We propose a machine-learning-based approach to predict the amount of calories from food images. Our system does not require any input from the user, except from an image of the food item. We take a pragmatic approach to accurately predict the amount of calories in a food item and solve the problem in three phases. First, we identify the type of the food item in the image. Second, we estimate the size of the food item in grams. Finally, by taking into consideration the output of the first two phases, we predict the amount of calories in the photographed food item. All these three phases are based purely on supervised machine-learning. We show that this pipelined approach is very effective in predicting the amount of calories in a food item as compared to baseline approaches which directly predicts the amount of calories from the image.},\n\turldate = {2019-08-02},\n\tbooktitle = {Proceedings of the {Thirty}-{First} {AAAI} {Conference} on {Artificial} {Intelligence}},\n\tpublisher = {AAAI Press},\n\tauthor = {Chokr, Manal and Elbassuoni, Shady},\n\tyear = {2017},\n\tpages = {4664--4669}\n}",
		"csl": {
			"publisher-place": "San Francisco, California, USA",
			"collection-title": "AAAI'17",
			"title": "Calories <span class=\"nocase\">Prediction</span> from <span class=\"nocase\">Food</span> <span class=\"nocase\">Images</span>",
			"URL": "http://dl.acm.org/citation.cfm?id=3297863.3297871",
			"container-title": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence",
			"publisher": "AAAI Press",
			"author": [
				{
					"given": "Manal",
					"family": "Chokr"
				},
				{
					"given": "Shady",
					"family": "Elbassuoni"
				}
			],
			"page": "4664-4669",
			"type": "paper-conference",
			"citation-label": "chokr_calories_2017",
			"id": "https://dl.acm.org/citation.cfm?id=3297871",
			"issued": {
				"date-parts": [
					[
						2017
					]
				]
			}
		}
	},
	"https://dl.acm.org/citation.cfm?doid=3126686.3126742": {
		"fetched": "2019-08-02T18:41:47.387Z",
		"bibtex": "\n@inproceedings{ege_image-based_2017,\n\taddress = {New York, NY, USA},\n\tseries = {Thematic {Workshops} '17},\n\ttitle = {Image-{Based} {Food} {Calorie} {Estimation} {Using} {Knowledge} on {Food} {Categories}, {Ingredients} and {Cooking} {Directions}},\n\tisbn = {9781450354165},\n\turl = {http://doi.acm.org/10.1145/3126686.3126742},\n\tdoi = {10.1145/3126686.3126742},\n\tabstract = {Image-based food calorie estimation is crucial to diverse mobile applications for recording everyday meal. However, some of them need human help for calorie estimation, and even if it is automatic, food categories are often limited or images from multiple viewpoints are required. Then, it is not yet achieved to estimate food calorie with practical accuracy and estimating food calories from a food photo is an unsolved problem. Therefore, in this paper, we propose estimating food calorie from a food photo by simultaneous learning of food calories, categories, ingredients and cooking directions using deep learning. Since there exists a strong correlation between food calories and food categories, ingredients and cooking directions information in general, we expect that simultaneous training of them brings performance boosting compared to independent single training. To this end, we use a multi-task CNN [1]. In addition, in this research, we construct two kinds of datasets that is a dataset of calorie-annotated recipe collected from Japanese recipe sites on the Web and a dataset collected from an American recipe site. In this experiment, we trained multi-task and single-task CNNs. As a result, the multi-task CNN achieved the better performance on both food category estimation and food calorie estimation than single-task CNNs. For the Japanese recipe dataset, by introducing a multi-task CNN, 0.039 were improved on the correlation coefficient, while for the American recipe dataset, 0.090 were raised compared to the result by the single-task CNN.},\n\turldate = {2019-08-02},\n\tbooktitle = {Proceedings of the on {Thematic} {Workshops} of {ACM} {Multimedia} 2017},\n\tpublisher = {ACM},\n\tauthor = {Ege, Takumi and Yanai, Keiji},\n\tyear = {2017},\n\tkeywords = {calorie estimation, food recognition, multi-task cnn},\n\tpages = {367--375}\n}",
		"csl": {
			"publisher-place": "New York, NY, USA",
			"collection-title": "Thematic Workshops '17",
			"title": "Image-<span class=\"nocase\">Based</span> <span class=\"nocase\">Food</span> <span class=\"nocase\">Calorie</span> <span class=\"nocase\">Estimation</span> <span class=\"nocase\">Using</span> <span class=\"nocase\">Knowledge</span> on <span class=\"nocase\">Food</span> <span class=\"nocase\">Categories</span>, <span class=\"nocase\">Ingredients</span> and <span class=\"nocase\">Cooking</span> <span class=\"nocase\">Directions</span>",
			"ISBN": "9781450354165",
			"URL": "http://doi.acm.org/10.1145/3126686.3126742",
			"DOI": "10.1145/3126686.3126742",
			"container-title": "Proceedings of the on Thematic Workshops of ACM Multimedia 2017",
			"publisher": "ACM",
			"author": [
				{
					"given": "Takumi",
					"family": "Ege"
				},
				{
					"given": "Keiji",
					"family": "Yanai"
				}
			],
			"page": "367-375",
			"type": "paper-conference",
			"citation-label": "ege_image-based_2017",
			"id": "https://dl.acm.org/citation.cfm?doid=3126686.3126742",
			"issued": {
				"date-parts": [
					[
						2017
					]
				]
			}
		}
	},
	"https://arxiv.org/abs/1812.06164": {
		"fetched": "2019-08-02T18:50:23.770Z",
		"bibtex": "\n@article{romero_inverse_2018,\n\ttitle = {Inverse {Cooking}: {Recipe} {Generation} from {Food} {Images}},\n\tshorttitle = {Inverse {Cooking}},\n\turl = {https://arxiv.org/abs/1812.06164v2},\n\tabstract = {People enjoy food photography because they appreciate food. Behind each meal\nthere is a story described in a complex recipe and, unfortunately, by simply\nlooking at a food image we do not have access to its preparation process.\nTherefore, in this paper we introduce an inverse cooking system that recreates\ncooking recipes given food images. Our system predicts ingredients as sets by\nmeans of a novel architecture, modeling their dependencies without imposing any\norder, and then generates cooking instructions by attending to both image and\nits inferred ingredients simultaneously. We extensively evaluate the whole\nsystem on the large-scale Recipe1M dataset and show that (1) we improve\nperformance w.r.t. previous baselines for ingredient prediction; (2) we are\nable to obtain high quality recipes by leveraging both image and ingredients;\n(3) our system is able to produce more compelling recipes than retrieval-based\napproaches according to human judgment. We make code and models publicly\navailable.},\n\tlanguage = {en},\n\turldate = {2019-08-02},\n\tauthor = {Romero, Adriana and Giro-i-Nieto, Xavier and Drozdzal, Michal and Salvador, Amaia},\n\tmonth = dec,\n\tyear = {2018}\n}",
		"csl": {
			"title": "Inverse <span class=\"nocase\">Cooking</span>: <span class=\"nocase\">Recipe</span> <span class=\"nocase\">Generation</span> from <span class=\"nocase\">Food</span> <span class=\"nocase\">Images</span>",
			"URL": "https://arxiv.org/abs/1812.06164v2",
			"language": "en",
			"author": [
				{
					"given": "Adriana",
					"family": "Romero"
				},
				{
					"given": "Xavier",
					"family": "Giro-i-Nieto"
				},
				{
					"given": "Michal",
					"family": "Drozdzal"
				},
				{
					"given": "Amaia",
					"family": "Salvador"
				}
			],
			"type": "article-journal",
			"citation-label": "romero_inverse_2018",
			"id": "https://arxiv.org/abs/1812.06164",
			"issued": {
				"date-parts": [
					[
						2018,
						12
					]
				]
			}
		}
	},
	"https://ndb.nal.usda.gov/ndb/": {
		"fetched": "2019-08-04T15:36:36.014Z",
		"bibtex": "\n@misc{noauthor_usda_nodate,\n\ttitle = {{USDA} {Food} {Composition} {Databases}},\n\turl = {https://ndb.nal.usda.gov/ndb/},\n\tabstract = {United States Department of Agriculture Food Composition Databases},\n\turldate = {2019-08-04},\n\tjournal = {ndb.nal.usda.gov}\n}",
		"csl": {
			"title": "<span class=\"nocase\">USDA</span> <span class=\"nocase\">Food</span> <span class=\"nocase\">Composition</span> <span class=\"nocase\">Databases</span>",
			"URL": "https://ndb.nal.usda.gov/ndb/",
			"container-title": "ndb.nal.usda.gov",
			"type": "webpage",
			"citation-label": "noauthor_usda_nodate",
			"id": "https://ndb.nal.usda.gov/ndb/"
		}
	},
	"https://arxiv.org/abs/1607.04606": {
		"fetched": "2019-08-04T15:57:12.817Z",
		"bibtex": "\n@article{mikolov_enriching_2016,\n\ttitle = {Enriching {Word} {Vectors} with {Subword} {Information}},\n\turl = {https://arxiv.org/abs/1607.04606v2},\n\tabstract = {Continuous word representations, trained on large unlabeled corpora are\nuseful for many natural language processing tasks. Popular models that learn\nsuch representations ignore the morphology of words, by assigning a distinct\nvector to each word. This is a limitation, especially for languages with large\nvocabularies and many rare words. In this paper, we propose a new approach\nbased on the skipgram model, where each word is represented as a bag of\ncharacter \\$n\\$-grams. A vector representation is associated to each character\n\\$n\\$-gram; words being represented as the sum of these representations. Our\nmethod is fast, allowing to train models on large corpora quickly and allows us\nto compute word representations for words that did not appear in the training\ndata. We evaluate our word representations on nine different languages, both on\nword similarity and analogy tasks. By comparing to recently proposed\nmorphological word representations, we show that our vectors achieve\nstate-of-the-art performance on these tasks.},\n\tlanguage = {en},\n\turldate = {2019-08-04},\n\tauthor = {Mikolov, Tomas and Joulin, Armand and Grave, Edouard and Bojanowski, Piotr},\n\tmonth = jul,\n\tyear = {2016}\n}",
		"csl": {
			"title": "Enriching <span class=\"nocase\">Word</span> <span class=\"nocase\">Vectors</span> with <span class=\"nocase\">Subword</span> <span class=\"nocase\">Information</span>",
			"URL": "https://arxiv.org/abs/1607.04606v2",
			"language": "en",
			"author": [
				{
					"given": "Tomas",
					"family": "Mikolov"
				},
				{
					"given": "Armand",
					"family": "Joulin"
				},
				{
					"given": "Edouard",
					"family": "Grave"
				},
				{
					"given": "Piotr",
					"family": "Bojanowski"
				}
			],
			"type": "article-journal",
			"citation-label": "mikolov_enriching_2016",
			"id": "https://arxiv.org/abs/1607.04606",
			"issued": {
				"date-parts": [
					[
						2016,
						7
					]
				]
			}
		}
	},
	"https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00051": {
		"fetched": "2019-08-04T15:57:47.701Z",
		"bibtex": "\n@article{bojanowski_enriching_2017,\n\ttitle = {Enriching {Word} {Vectors} with {Subword} {Information}},\n\tvolume = {5},\n\turl = {https://doi.org/10.1162/tacl_a_00051},\n\tdoi = {10.1162/tacl_a_00051},\n\tabstract = {Continuous word representations, trained on large unlabeled corpora are useful                     for many natural language processing tasks. Popular models that learn such                     representations ignore the morphology of words, by assigning a distinct vector                     to each word. This is a limitation, especially for languages with large                     vocabularies and many rare words. In this paper, we propose a new approach based                     on the skipgram model, where each word is represented as a bag of character                         n-grams. A vector representation is associated                     to each character n-gram; words being represented                     as the sum of these representations. Our method is fast, allowing to train                     models on large corpora quickly and allows us to compute word representations                     for words that did not appear in the training data. We evaluate our word                     representations on nine different languages, both on word similarity and analogy                     tasks. By comparing to recently proposed morphological word representations, we                     show that our vectors achieve state-of-the-art performance on these tasks.},\n\turldate = {2019-08-04},\n\tjournal = {Transactions of the Association for Computational Linguistics},\n\tauthor = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},\n\tmonth = dec,\n\tyear = {2017},\n\tpages = {135--146}\n}",
		"csl": {
			"title": "Enriching <span class=\"nocase\">Word</span> <span class=\"nocase\">Vectors</span> with <span class=\"nocase\">Subword</span> <span class=\"nocase\">Information</span>",
			"volume": "5",
			"URL": "https://doi.org/10.1162/tacl_a_00051",
			"DOI": "10.1162/tacl_a_00051",
			"container-title": "Transactions of the Association for Computational Linguistics",
			"author": [
				{
					"given": "Piotr",
					"family": "Bojanowski"
				},
				{
					"given": "Edouard",
					"family": "Grave"
				},
				{
					"given": "Armand",
					"family": "Joulin"
				},
				{
					"given": "Tomas",
					"family": "Mikolov"
				}
			],
			"page": "135-146",
			"type": "article-journal",
			"citation-label": "bojanowski_enriching_2017",
			"id": "https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00051",
			"issued": {
				"date-parts": [
					[
						2017,
						12
					]
				]
			}
		}
	},
	"https://ai.google/research/pubs/pub46808/": {
		"fetched": "2019-08-04T16:02:04.549Z",
		"bibtex": "\n@inproceedings{cer_universal_2018,\n\taddress = {Brussels, Belgium},\n\ttitle = {Universal {Sentence} {Encoder}},\n\turl = {https://arxiv.org/abs/1803.11175},\n\turldate = {2019-08-04},\n\tbooktitle = {In submission to: {EMNLP} demonstration},\n\tauthor = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole Lyn Untalan and John, Rhomni St and Constant, Noah and Guajardo-Céspedes, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-hsuan and Strope, Brian and Kurzweil, Ray},\n\tyear = {2018}\n}",
		"csl": {
			"publisher-place": "Brussels, Belgium",
			"title": "Universal <span class=\"nocase\">Sentence</span> <span class=\"nocase\">Encoder</span>",
			"URL": "https://arxiv.org/abs/1803.11175",
			"container-title": "In submission to: EMNLP demonstration",
			"author": [
				{
					"given": "Daniel",
					"family": "Cer"
				},
				{
					"given": "Yinfei",
					"family": "Yang"
				},
				{
					"given": "Sheng-yi",
					"family": "Kong"
				},
				{
					"given": "Nan",
					"family": "Hua"
				},
				{
					"given": "Nicole Lyn Untalan",
					"family": "Limtiaco"
				},
				{
					"given": "Rhomni St",
					"family": "John"
				},
				{
					"given": "Noah",
					"family": "Constant"
				},
				{
					"given": "Mario",
					"family": "Guajardo-Céspedes"
				},
				{
					"given": "Steve",
					"family": "Yuan"
				},
				{
					"given": "Chris",
					"family": "Tar"
				},
				{
					"given": "Yun-hsuan",
					"family": "Sung"
				},
				{
					"given": "Brian",
					"family": "Strope"
				},
				{
					"given": "Ray",
					"family": "Kurzweil"
				}
			],
			"type": "paper-conference",
			"citation-label": "cer_universal_2018",
			"id": "https://ai.google/research/pubs/pub46808/",
			"issued": {
				"date-parts": [
					[
						2018
					]
				]
			}
		}
	},
	"https://ieeexplore.ieee.org/abstract/document/6123373": {
		"fetched": "2019-08-10T10:38:06.672Z",
		"bibtex": "\n@inproceedings{miyazaki_image-based_2011,\n\ttitle = {Image-based {Calorie} {Content} {Estimation} for {Dietary} {Assessment}},\n\turl = {https://ieeexplore.ieee.org/abstract/document/6123373},\n\tdoi = {10.1109/ISM.2011.66},\n\tabstract = {In this paper, we present an image-analysis based approach to calorie content estimation for dietary assessment. We make use of daily food images captured and stored by multiple users in a public Web service called FoodLog. The images are taken without any control or markers. We build a dictionary dataset of 6512 images contained in FoodLog the calorie content of which have been estimated by experts in nutrition. An image is compared to the ground truth data from the point of views of multiple image features such as color histograms, color correlograms and SURF fetures, and the ground truth images are ranked by similarities. Finally, calorie content of the input food image is computed by linear estimation using the top n ranked calories in multiple features. The distribution of the estimation shows that 79\\% of the estimations are correct within ±40\\% error and 35\\% correct within ±20\\% error.},\n\turldate = {2019-08-10},\n\tbooktitle = {2011 {IEEE} {International} {Symposium} on {Multimedia}},\n\tauthor = {Miyazaki, T. and Silva, G. C. de and Aizawa, K.},\n\tmonth = dec,\n\tyear = {2011},\n\tkeywords = {health care, image colour analysis, Web services, image-based calorie content estimation, dietary assessment, image-analysis, daily food image, public Web service, FoodLog, color histogram, color correlogram, SURF feature, ground truth image, linear estimation, dietary management system, Estimation, Image color analysis, Feature extraction, Accuracy, Histograms, Correlation, Dictionaries, Calorie content estimation, FoodLog, Image processing, LifeLog},\n\tpages = {363--368}\n}",
		"csl": {
			"title": "Image-based <span class=\"nocase\">Calorie</span> <span class=\"nocase\">Content</span> <span class=\"nocase\">Estimation</span> for <span class=\"nocase\">Dietary</span> <span class=\"nocase\">Assessment</span>",
			"URL": "https://ieeexplore.ieee.org/abstract/document/6123373",
			"DOI": "10.1109/ISM.2011.66",
			"container-title": "2011 IEEE International Symposium on Multimedia",
			"author": [
				{
					"given": "T.",
					"family": "Miyazaki"
				},
				{
					"given": "G. C. de",
					"family": "Silva"
				},
				{
					"given": "K.",
					"family": "Aizawa"
				}
			],
			"page": "363-368",
			"type": "paper-conference",
			"citation-label": "miyazaki_image-based_2011",
			"id": "https://ieeexplore.ieee.org/abstract/document/6123373",
			"issued": {
				"date-parts": [
					[
						2011,
						12
					]
				]
			}
		}
	},
	"https://ieeexplore.ieee.org/abstract/document/8666636": {
		"fetched": "2019-08-10T10:51:05.562Z",
		"bibtex": "\n@article{subhi_vision-based_2019,\n\ttitle = {Vision-{Based} {Approaches} for {Automatic} {Food} {Recognition} and {Dietary} {Assessment}: {A} {Survey}},\n\tvolume = {7},\n\tissn = {2169-3536},\n\tshorttitle = {Vision-{Based} {Approaches} for {Automatic} {Food} {Recognition} and {Dietary} {Assessment}},\n\turl = {https://ieeexplore.ieee.org/abstract/document/8666636},\n\tdoi = {10.1109/ACCESS.2019.2904519},\n\tabstract = {Consuming the proper amount and right type of food have been the concern of many dieticians and healthcare conventions. In addition to physical activity and exercises, maintaining a healthy diet is necessary to avoid obesity and other health-related issues, such as diabetes, stroke, and many cardiovascular diseases. Recent advancements in machine learning applications and technologies have made it possible to develop automatic or semi-automatic dietary assessment solutions, which is a more convenient approach to monitor daily food intake and control eating habits. These solutions aim to address the issues found in the traditional dietary monitoring systems that suffer from imprecision, underreporting, time consumption, and low adherence. In this paper, the recent vision-based approaches and techniques have been widely explored to outline the current approaches and methodologies used for automatic dietary assessment, their performances, feasibility, and unaddressed challenges and issues.},\n\turldate = {2019-08-10},\n\tjournal = {IEEE Access},\n\tauthor = {Subhi, M. A. and Ali, S. H. and Mohammed, M. A.},\n\tyear = {2019},\n\tkeywords = {computer vision, food products, food safety, health care, learning (artificial intelligence), medical computing, patient monitoring, machine learning applications, automatic dietary assessment, healthcare conventions, physical activity, food intake monitoring, vision-based approaches, automatic food recognition, food consumption, Image segmentation, Feature extraction, Classification algorithms, Training, Estimation, Machine learning, Image recognition, Food recognition, food classification, food volume estimation, food nutrient information, food image datasets},\n\tpages = {35370--35381}\n}",
		"csl": {
			"title": "Vision-<span class=\"nocase\">Based</span> <span class=\"nocase\">Approaches</span> for <span class=\"nocase\">Automatic</span> <span class=\"nocase\">Food</span> <span class=\"nocase\">Recognition</span> and <span class=\"nocase\">Dietary</span> <span class=\"nocase\">Assessment</span>: <span class=\"nocase\">A</span> <span class=\"nocase\">Survey</span>",
			"volume": "7",
			"ISSN": "2169-3536",
			"URL": "https://ieeexplore.ieee.org/abstract/document/8666636",
			"DOI": "10.1109/ACCESS.2019.2904519",
			"container-title": "IEEE Access",
			"author": [
				{
					"given": "M. A.",
					"family": "Subhi"
				},
				{
					"given": "S. H.",
					"family": "Ali"
				},
				{
					"given": "M. A.",
					"family": "Mohammed"
				}
			],
			"page": "35370-35381",
			"type": "article-journal",
			"citation-label": "subhi_vision-based_2019",
			"id": "https://ieeexplore.ieee.org/abstract/document/8666636",
			"issued": {
				"date-parts": [
					[
						2019
					]
				]
			}
		}
	},
	"https://ieeexplore.ieee.org/document/7410503": {
		"fetched": "2019-08-10T10:57:03.104Z",
		"bibtex": "\n@inproceedings{myers_im2calories:_2015,\n\ttitle = {Im2Calories: {Towards} an {Automated} {Mobile} {Vision} {Food} {Diary}},\n\tshorttitle = {Im2Calories},\n\turl = {https://ieeexplore.ieee.org/document/7410503},\n\tdoi = {10.1109/ICCV.2015.146},\n\tabstract = {We present a system which can recognize the contents of your meal from a single image, and then predict its nutritional contents, such as calories. The simplest version assumes that the user is eating at a restaurant for which we know the menu. In this case, we can collect images offline to train a multi-label classifier. At run time, we apply the classifier (running on your phone) to predict which foods are present in your meal, and we lookup the corresponding nutritional facts. We apply this method to a new dataset of images from 23 different restaurants, using a CNN-based classifier, significantly outperforming previous work. The more challenging setting works outside of restaurants. In this case, we need to estimate the size of the foods, as well as their labels. This requires solving segmentation and depth / volume estimation from a single image. We present CNN-based approaches to these problems, with promising preliminary results.},\n\turldate = {2019-08-10},\n\tbooktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},\n\tauthor = {Myers, A. and Johnston, N. and Rathod, V. and Korattikara, A. and Gorban, A. and Silberman, N. and Guadarrama, S. and Papandreou, G. and Huang, J. and Murphy, K.},\n\tmonth = dec,\n\tyear = {2015},\n\tkeywords = {computer vision, image classification, image segmentation, medical computing, mobile computing, neural nets, personal computing, prediction theory, Im2Calories, automated mobile vision food diary, content recognition, nutritional content prediction, restaurant, multilabel classifier, food prediction, nutritional facts, CNN-based classifier, food size estimation, depth estimation, volume estimation, Image segmentation, Mobile communication, Visualization, Image recognition, Cameras, Machine learning},\n\tpages = {1233--1241}\n}",
		"csl": {
			"title": "Im2Calories: <span class=\"nocase\">Towards</span> an <span class=\"nocase\">Automated</span> <span class=\"nocase\">Mobile</span> <span class=\"nocase\">Vision</span> <span class=\"nocase\">Food</span> <span class=\"nocase\">Diary</span>",
			"URL": "https://ieeexplore.ieee.org/document/7410503",
			"DOI": "10.1109/ICCV.2015.146",
			"container-title": "2015 IEEE International Conference on Computer Vision (ICCV)",
			"author": [
				{
					"given": "A.",
					"family": "Myers"
				},
				{
					"given": "N.",
					"family": "Johnston"
				},
				{
					"given": "V.",
					"family": "Rathod"
				},
				{
					"given": "A.",
					"family": "Korattikara"
				},
				{
					"given": "A.",
					"family": "Gorban"
				},
				{
					"given": "N.",
					"family": "Silberman"
				},
				{
					"given": "S.",
					"family": "Guadarrama"
				},
				{
					"given": "G.",
					"family": "Papandreou"
				},
				{
					"given": "J.",
					"family": "Huang"
				},
				{
					"given": "K.",
					"family": "Murphy"
				}
			],
			"page": "1233-1241",
			"type": "paper-conference",
			"citation-label": "myers_im2calories:_2015",
			"id": "https://ieeexplore.ieee.org/document/7410503",
			"issued": {
				"date-parts": [
					[
						2015,
						12
					]
				]
			}
		}
	},
	"https://caloriemama.ai/": {
		"fetched": "2019-08-17T15:30:39.071Z",
		"bibtex": "\n@misc{noauthor_calorie_nodate,\n\ttitle = {Calorie {Mama} {Food} {AI} - {Food} {Image} {Recognition} and {Calorie} {Counter} using {Deep} {Learning}},\n\turl = {https://caloriemama.ai/},\n\tabstract = {Calorie Mama makes instant nutrition and calorie estimates from your meals - just snap a food photo and let Mama do the rest. The app uses computer vision and deep learning to classify thousand of food categories from cuisines all around the world. Calorie Mama Food AI API (Smart Nutrition Analysis Platform) are developed by Azumio, Inc. },\n\turldate = {2019-08-17},\n\tjournal = {caloriemama.ai}\n}",
		"csl": {
			"title": "Calorie <span class=\"nocase\">Mama</span> <span class=\"nocase\">Food</span> <span class=\"nocase\">AI</span> - <span class=\"nocase\">Food</span> <span class=\"nocase\">Image</span> <span class=\"nocase\">Recognition</span> and <span class=\"nocase\">Calorie</span> <span class=\"nocase\">Counter</span> using <span class=\"nocase\">Deep</span> <span class=\"nocase\">Learning</span>",
			"URL": "https://caloriemama.ai/",
			"container-title": "caloriemama.ai",
			"type": "webpage",
			"citation-label": "noauthor_calorie_nodate",
			"id": "https://caloriemama.ai/"
		}
	},
	"https://ieeexplore.ieee.org/document/7780459": {
		"fetched": "2019-08-17T17:21:51.601Z",
		"bibtex": "\n@inproceedings{he_deep_2016,\n\ttitle = {Deep {Residual} {Learning} for {Image} {Recognition}},\n\turl = {https://ieeexplore.ieee.org/document/7780459},\n\tdoi = {10.1109/CVPR.2016.90},\n\tabstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\\&\\#x00d7; deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \\&amp; COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},\n\turldate = {2019-08-17},\n\tbooktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},\n\tauthor = {He, K. and Zhang, X. and Ren, S. and Sun, J.},\n\tmonth = jun,\n\tyear = {2016},\n\tkeywords = {image classification, learning (artificial intelligence), neural nets, object detection, COCO segmentation, ImageNet localization, ILSVRC \\&amp, COCO 2015 competitions, deep residual nets, COCO object detection dataset, visual recognition tasks, CIFAR-10, ILSVRC 2015 classification task, ImageNet test set, VGG nets, residual nets, ImageNet dataset, residual function learning, deeper neural network training, image recognition, deep residual learning, Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation},\n\tpages = {770--778}\n}",
		"csl": {
			"title": "Deep <span class=\"nocase\">Residual</span> <span class=\"nocase\">Learning</span> for <span class=\"nocase\">Image</span> <span class=\"nocase\">Recognition</span>",
			"URL": "https://ieeexplore.ieee.org/document/7780459",
			"DOI": "10.1109/CVPR.2016.90",
			"container-title": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
			"author": [
				{
					"given": "K.",
					"family": "He"
				},
				{
					"given": "X.",
					"family": "Zhang"
				},
				{
					"given": "S.",
					"family": "Ren"
				},
				{
					"given": "J.",
					"family": "Sun"
				}
			],
			"page": "770-778",
			"type": "paper-conference",
			"citation-label": "he_deep_2016",
			"id": "https://ieeexplore.ieee.org/document/7780459",
			"issued": {
				"date-parts": [
					[
						2016,
						6
					]
				]
			}
		}
	},
	"https://ieeexplore.ieee.org/document/8099726": {
		"fetched": "2019-08-17T17:21:53.041Z",
		"bibtex": "\n@inproceedings{huang_densely_2017,\n\ttitle = {Densely {Connected} {Convolutional} {Networks}},\n\turl = {https://ieeexplore.ieee.org/document/8099726},\n\tdoi = {10.1109/CVPR.2017.243},\n\tabstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections-one between each layer and its subsequent layer-our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.},\n\turldate = {2019-08-17},\n\tbooktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},\n\tauthor = {Huang, G. and Liu, Z. and Maaten, L. v d and Weinberger, K. Q.},\n\tmonth = jul,\n\tyear = {2017},\n\tkeywords = {convolution, feedforward neural nets, learning (artificial intelligence), DenseNet, traditional convolutional networks, feature propagation, feature reuse, object recognition benchmark tasks, dense convolutional network, vanishing-gradient problem, Training, Convolution, Network architecture, Convolutional codes, Neural networks, Road transportation},\n\tpages = {2261--2269}\n}",
		"csl": {
			"title": "Densely <span class=\"nocase\">Connected</span> <span class=\"nocase\">Convolutional</span> <span class=\"nocase\">Networks</span>",
			"URL": "https://ieeexplore.ieee.org/document/8099726",
			"DOI": "10.1109/CVPR.2017.243",
			"container-title": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
			"author": [
				{
					"given": "G.",
					"family": "Huang"
				},
				{
					"given": "Z.",
					"family": "Liu"
				},
				{
					"given": "L. v d",
					"family": "Maaten"
				},
				{
					"given": "K. Q.",
					"family": "Weinberger"
				}
			],
			"page": "2261-2269",
			"type": "paper-conference",
			"citation-label": "huang_densely_2017",
			"id": "https://ieeexplore.ieee.org/document/8099726",
			"issued": {
				"date-parts": [
					[
						2017,
						7
					]
				]
			}
		}
	},
	"https://arxiv.org/abs/1611.05431": {
		"fetched": "2019-08-18T16:42:14.464Z",
		"bibtex": "\n@article{he_aggregated_2016,\n\ttitle = {Aggregated {Residual} {Transformations} for {Deep} {Neural} {Networks}},\n\turl = {https://arxiv.org/abs/1611.05431v2},\n\tabstract = {We present a simple, highly modularized network architecture for image\nclassification. Our network is constructed by repeating a building block that\naggregates a set of transformations with the same topology. Our simple design\nresults in a homogeneous, multi-branch architecture that has only a few\nhyper-parameters to set. This strategy exposes a new dimension, which we call\n\"cardinality\" (the size of the set of transformations), as an essential factor\nin addition to the dimensions of depth and width. On the ImageNet-1K dataset,\nwe empirically show that even under the restricted condition of maintaining\ncomplexity, increasing cardinality is able to improve classification accuracy.\nMoreover, increasing cardinality is more effective than going deeper or wider\nwhen we increase the capacity. Our models, named ResNeXt, are the foundations\nof our entry to the ILSVRC 2016 classification task in which we secured 2nd\nplace. We further investigate ResNeXt on an ImageNet-5K set and the COCO\ndetection set, also showing better results than its ResNet counterpart. The\ncode and models are publicly available online.},\n\tlanguage = {en},\n\turldate = {2019-08-18},\n\tauthor = {He, Kaiming and Tu, Zhuowen and Dollár, Piotr and Girshick, Ross and Xie, Saining},\n\tmonth = nov,\n\tyear = {2016}\n}",
		"csl": {
			"title": "Aggregated <span class=\"nocase\">Residual</span> <span class=\"nocase\">Transformations</span> for <span class=\"nocase\">Deep</span> <span class=\"nocase\">Neural</span> <span class=\"nocase\">Networks</span>",
			"URL": "https://arxiv.org/abs/1611.05431v2",
			"language": "en",
			"author": [
				{
					"given": "Kaiming",
					"family": "He"
				},
				{
					"given": "Zhuowen",
					"family": "Tu"
				},
				{
					"given": "Piotr",
					"family": "Dollár"
				},
				{
					"given": "Ross",
					"family": "Girshick"
				},
				{
					"given": "Saining",
					"family": "Xie"
				}
			],
			"type": "article-journal",
			"citation-label": "he_aggregated_2016",
			"id": "https://arxiv.org/abs/1611.05431",
			"issued": {
				"date-parts": [
					[
						2016,
						11
					]
				]
			}
		}
	}
}